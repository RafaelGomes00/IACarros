Steps,Policy/Entropy,Policy/Extrinsic Value Estimate,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Environment/Episode Length,Environment/Cumulative Reward,Policy/Extrinsic Reward,Is Training
10000,2.1830893,-1.1549885,5.464862,0.061242532,0.0002999685,4999.0,-172.0,-172.0,1.0
20000,2.1807916,-1.3915635,17.789082,0.07522552,0.00029991212,4999.0,-841.0,-841.0,1.0
30000,2.1397834,-2.5592203,7.547591,0.06663179,0.0002998495,4999.0,-375.0,-375.0,1.0
40000,2.1658587,-1.9586062,2.2616882,0.06762194,0.00029978677,4999.0,-231.0,-231.0,1.0
50000,2.1260893,-2.2996502,3.1004395,0.07198954,0.00029973043,4999.0,-230.5,-230.5,1.0
60000,2.1285386,-2.1889591,3.8589115,0.07230173,0.00029967402,4999.0,-94.0,-94.0,1.0
70000,2.0068996,-2.0054657,1.3934106,0.06796224,0.00029961136,4999.0,-139.5,-139.5,1.0
80000,1.6715859,-1.6113975,0.9715376,0.064882204,0.0002995487,4999.0,-31.0,-31.0,1.0
90000,1.5075634,-0.88833326,0.40966773,0.073868535,0.00029948604,4999.0,12.5,12.5,1.0
100000,1.5594014,-0.38115016,0.40499368,0.07785882,0.00029942967,4999.0,18.0,18.0,1.0
110000,1.4252639,-0.08791617,1.2737753,0.063909106,0.0002993733,4999.0,-27.5,-27.5,1.0
120000,1.472177,-0.18419705,0.5672892,0.066138595,0.00029931057,4999.0,-0.5,-0.5,1.0
