Steps,Policy/Extrinsic Value Estimate,Policy/Entropy,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Environment/Episode Length,Environment/Cumulative Reward,Policy/Extrinsic Reward,Is Training
30000,8.075542,0.7742145,72.75127,0.06994223,0.0002998471,1999.0,625.1329270723191,625.1329270723191,1.0
40000,3.9041507,1.2321014,87.801926,0.06381703,0.00029978988,None,None,None,1.0
50000,7.1290073,0.90483207,45.322,0.07088741,0.00029973223,1999.0,620.9311933761293,620.9311933761293,1.0
60000,5.435636,0.8594045,89.621086,0.07098065,0.00029966843,None,None,None,1.0
70000,5.598205,1.0929388,21.33072,0.0660745,0.00029961113,1999.0,616.0388159616427,616.0388159616427,1.0
80000,7.671124,0.6834853,97.598694,0.06691384,0.00029955333,None,None,None,1.0
90000,2.6940246,1.3079699,44.505043,0.068612136,0.00029948974,1999.0,342.8216274773533,342.8216274773533,1.0
100000,9.437881,0.870886,109.3969,0.06603713,0.00029942556,None,None,None,1.0
110000,4.0204005,1.4472545,5.6871037,0.06859153,0.00029936855,1999.0,595.0677338540554,595.0677338540554,1.0
120000,11.847362,0.77030545,86.05635,0.0693916,0.00029931046,None,594.7700404524803,594.7700404524803,1.0
130000,5.8356996,1.5191042,25.826477,0.07332905,0.00029924707,None,None,None,1.0
