Steps,Policy/Entropy,Policy/Extrinsic Value Estimate,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Environment/Episode Length,Environment/Cumulative Reward,Policy/Extrinsic Reward,Is Training
10000,2.1911001,0.11010736,10364115.0,0.07758742,0.0002999685,4140.0,-49994.5,-49994.5,1.0
20000,2.0929396,-0.73404926,11862238.0,0.05901336,0.00029991186,1842.6666666666667,-49967.833333333336,-49967.833333333336,1.0
30000,2.089295,-3.7539608,31551450.0,0.061411165,0.00029984926,1456.7142857142858,-42815.71428571428,-42815.71428571428,1.0
40000,2.124692,-6.3565745,26581420.0,0.06802833,0.0002997871,2065.75,-99978.25,-99978.25,1.0
50000,2.1220217,-12.5076885,10501319.0,0.06735895,0.00029973048,3661.6666666666665,-33309.333333333336,-33309.333333333336,1.0
60000,2.1177087,-12.738949,14805931.0,0.06939191,0.00029967405,1171.375,-37478.625,-37478.625,1.0
70000,2.0227177,-17.369507,8430798.0,0.062010117,0.0002996113,1365.5,-24970.75,-24970.75,1.0
80000,1.9280273,-20.367596,34491350.0,0.06898026,0.00029954888,1745.5,-83285.5,-83285.5,1.0
