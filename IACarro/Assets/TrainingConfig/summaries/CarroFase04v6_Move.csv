Steps,Policy/Extrinsic Value Estimate,Policy/Entropy,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Environment/Episode Length,Environment/Cumulative Reward,Policy/Extrinsic Reward,Is Training
30000,2.0166063,0.8329286,120.553795,0.069699004,0.0002998471,1999.0,296.41945604031736,296.41945604031736,1.0
40000,1.0355122,0.94644773,26.058372,0.070365354,0.00029978988,None,None,None,1.0
50000,1.3702053,0.9076162,117.8943,0.07484111,0.00029973223,1999.0,302.6121718321334,302.6121718321334,1.0
60000,3.4085426,0.4968926,11.988451,0.06637584,0.00029966843,None,None,None,1.0
70000,2.9220724,0.90194535,30.445621,0.067350954,0.00029961113,1999.0,472.62132241509175,472.62132241509175,1.0
80000,3.3223157,0.7287268,100.47706,0.065097064,0.00029955333,None,None,None,1.0
90000,3.857646,0.73339134,25.06869,0.06886891,0.00029948974,1999.0,481.88491566682404,481.88491566682404,1.0
100000,4.4608274,0.5977243,115.2122,0.0637183,0.00029942556,None,None,None,1.0
110000,3.6288512,0.6004828,15.067549,0.06533826,0.00029936855,1999.0,483.9970950804651,483.9970950804651,1.0
120000,4.853026,0.6841922,116.89628,0.07442351,0.00029931046,None,445.81994356215,445.81994356215,1.0
130000,4.521,0.6901613,19.643408,0.06779407,0.00029924707,None,None,None,1.0
